{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Colab\n",
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Stable Baselines, Gym, and Pytao\n",
    "!apt-get install ffmpeg freeglut3-dev xvfb\n",
    "!pip install \"stable-baselines3[extra]>=2.0.0a4\"\n",
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge pytao\n",
    "!conda install -c conda-forge bmad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import environ as en\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from stable_baselines3 import SAC\n",
    "#from stable_baselines3 import PPO\n",
    "#from stable_baselines3 import TD3\n",
    "from stable_baselines3.sac.policies import MlpPolicy\n",
    "#from stable_baselines3.ppo.policies import MlpPolicy\n",
    "#from stable_baselines3.td3.policies import MlpPolicy\n",
    "import pytao\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num of updates = max_steps_per_episode / num_steps_in_model\n",
    "\n",
    "# Hyperparameters (Default)\n",
    "model_type = \"SAC\" #############################################################\n",
    "max_steps_per_episode = 128\n",
    "num_episodes = 10\n",
    "steps_to_train = 128\n",
    "num_steps_in_model = 64 # PPO Hyp\n",
    "train_freq = 1 # SAC #Hyp\n",
    "gradient_steps = 1 # SAC  #Hyp\n",
    "learning_rate = 0.0001 #Hyp\n",
    "gamma = 0.99 # SAC #Hyp\n",
    "tau = 0.005 # SAC  #Hyp\n",
    "gae_lambda = 0.95 # PPO Hyp\n",
    "batch_size = 64   #Hyp\n",
    "determ = True\n",
    "my_convergence_val = 0.005 ##################################################################################\n",
    "my_bad_convergence_val = 0.01 # Hyp\n",
    "min_step_value = 5 # Like Training convergence\n",
    "my_reward_alpha = 1.0 # Hyp\n",
    "my_reward_beta = 1.0 # Hyp\n",
    "my_reward_fun = 4 ####################################################################\n",
    "what_to_do = \"y_only\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an enviroment for the model\n",
    "\n",
    "if what_to_do == \"x_and_y\":\n",
    "  env = en.LTBEnv(\"tao.init\",\n",
    "                  {\"orbit.x\": \"MW\", \"orbit.y\": \"MW\"},\n",
    "                  [\"correctors_x\", \"correctors_y\"],\n",
    "                  -0.01,\n",
    "                  0.01,\n",
    "                  max_steps = max_steps_per_episode,\n",
    "                  convergence_val = my_convergence_val,\n",
    "                  bad_convergence_val = my_bad_convergence_val,\n",
    "                  reward_alpha = my_reward_alpha,\n",
    "                  reward_beta = my_reward_beta,\n",
    "                  reward_fun = my_reward_fun)\n",
    "elif what_to_do == \"x_only\":\n",
    "  env = en.LTBEnv(\"tao.init\",\n",
    "                  {\"orbit.x\": \"MW\"},\n",
    "                  [\"correctors_x\"],\n",
    "                  -0.01,\n",
    "                  0.01,\n",
    "                  max_steps = max_steps_per_episode,\n",
    "                  convergence_val = my_convergence_val,\n",
    "                  bad_convergence_val = my_bad_convergence_val,\n",
    "                  reward_alpha = my_reward_alpha,\n",
    "                  reward_beta = my_reward_beta,\n",
    "                  reward_fun = my_reward_fun)\n",
    "elif what_to_do == \"y_only\":\n",
    "  env = en.LTBEnv(\"tao.init\",\n",
    "                  {\"orbit.y\": \"MW\"},\n",
    "                  [\"correctors_y\"],\n",
    "                  -0.01,\n",
    "                  0.01,\n",
    "                  max_steps = max_steps_per_episode,\n",
    "                  convergence_val = my_convergence_val,\n",
    "                  bad_convergence_val = my_bad_convergence_val,\n",
    "                  reward_alpha = my_reward_alpha,\n",
    "                  reward_beta = my_reward_beta,\n",
    "                  reward_fun = my_reward_fun)\n",
    "\n",
    "# Make the model\n",
    "\n",
    "## PPO\n",
    "if model_type == \"PPO\":\n",
    "  model = PPO(MlpPolicy, env, verbose=0, n_steps = num_steps_in_model, learning_rate = learning_rate, batch_size = batch_size, gamma = gamma, gae_lambda = gae_lambda)\n",
    "  random_model = PPO(MlpPolicy, env, verbose=0, n_steps = num_steps_in_model, learning_rate = learning_rate, batch_size = batch_size, gamma = gamma, gae_lambda = gae_lambda)\n",
    "\n",
    "## SAC\n",
    "if model_type == \"SAC\":\n",
    "  model = SAC(MlpPolicy, env, verbose=0, train_freq = train_freq, gradient_steps = gradient_steps, learning_rate = learning_rate, batch_size = batch_size, gamma = gamma, tau = tau)\n",
    "  random_model = SAC(MlpPolicy, env, verbose=0, train_freq = train_freq, gradient_steps = gradient_steps, learning_rate = learning_rate, batch_size = batch_size, gamma = gamma, tau = tau)\n",
    "\n",
    "## TD3\n",
    "if model_type == \"TD3\":\n",
    "  model = TD3(MlpPolicy, env, verbose=0, train_freq = train_freq, gradient_steps = gradient_steps, learning_rate = learning_rate, batch_size = batch_size, gamma = gamma, tau = tau)\n",
    "  random_model = TD3(MlpPolicy, env, verbose=0, train_freq = train_freq, gradient_steps = gradient_steps, learning_rate = learning_rate, batch_size = batch_size, gamma = gamma, tau = tau)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "steps_per_episode = []\n",
    "max_state_per_episode = []\n",
    "average_rewards = []\n",
    "final_action_list = []\n",
    "\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "  # Train the model\n",
    "  model.learn(total_timesteps = steps_to_train, progress_bar = True)\n",
    "\n",
    "  state, _ = env.reset()\n",
    "  state = state + np.random.uniform(-0.001, 0.001, size = state.shape)\n",
    "  step = 0\n",
    "\n",
    "  # Replay Buffer\n",
    "  states = [state]\n",
    "  action_list = []\n",
    "  rewards = []\n",
    "  dones = []\n",
    "  truncateds = []\n",
    "\n",
    "  # Test the convergence of the model\n",
    "  while True:\n",
    "      action, _ = model.predict(state, deterministic=determ)\n",
    "      new_state, reward, done, truncated, _ = env.step(action)\n",
    "      states.append(new_state), rewards.append(reward)\n",
    "      dones.append(done), truncateds.append(truncated), action_list.append(action)\n",
    "\n",
    "      step += 1\n",
    "\n",
    "      state = new_state\n",
    "      if done:\n",
    "        break\n",
    "\n",
    "  print(step)\n",
    "  max_state_at_end = np.max(np.abs(states[-1]))\n",
    "  avg_reward = np.mean(rewards)\n",
    "  average_rewards.append(avg_reward)\n",
    "  steps_per_episode.append(step)\n",
    "  max_state_per_episode.append(max_state_at_end)\n",
    "  final_action_list.append(action_list[-1])\n",
    "  print(max_state_at_end)\n",
    "\n",
    "  # Converged finally\n",
    "  if max_state_at_end <= my_convergence_val:\n",
    "    break\n",
    "\n",
    "episode_array = np.arange(start = 1, stop = episode+2, step = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, step_val_epis in enumerate(steps_per_episode):\n",
    "  print(f\"Max Steps To Converge for Episode {i+1} is {step_val_epis}\")\n",
    "\n",
    "plt.plot(episode_array, steps_per_episode)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Number of steps taken\")\n",
    "plt.title(\"Convergence Rate Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, final_action in enumerate(final_action_list):\n",
    "  print(f\"Final Corrector Values for Episode {i+1} are {final_action}\")\n",
    "  print(f\"Final Max Orbit for Episode {i+1} are {max_state_per_episode[i]}\")\n",
    "\n",
    "print(f\"\\nMax Orbit for Initial State is {np.max(np.abs(env.initial_state))}\")\n",
    "print(f\"Biggest Difference of Max Orbit is {np.max(np.abs(env.initial_state)) - np.min(max_state_per_episode)}\")\n",
    "\n",
    "plt.plot(episode_array, max_state_per_episode)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Max Orbit Value\")\n",
    "plt.title(\"Max Orbit Value at end of Episode\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only use to combine actions\n",
    "x_action = [-0.00020242,  0.00029166, -0.00033901,  0.00074605, -0.00028885,  0.00035161]\n",
    "y_action = [ 1.73087597e-03,  1.42020226e-03, -6.98799491e-04, -1.31446242e-03,\n",
    " -1.12302303e-04, -6.85226917e-05, -3.06529999e-04]\n",
    "\n",
    "combined_action = x_action + y_action\n",
    "print(combined_action)\n",
    "tao = env.tao\n",
    "\n",
    "en.set_new_variable_values(tao, [\"correctors_x\"], x_action)\n",
    "en.set_new_variable_values(tao, [\"correctors_y\"], y_action)\n",
    "print(f\"Max Orbit: {np.max(np.abs(en.get_data_values(tao, {'orbit.x': 'MW', 'orbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, avg_rew_epi in enumerate(average_rewards):\n",
    "  print(f\"Average Reward for Episode {i+1} is {avg_rew_epi}\")\n",
    "\n",
    "plt.plot(episode_array, average_rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.title(\"Reward Plot\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
